{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6131005,"sourceType":"datasetVersion","datasetId":3515188},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045,"modelId":575}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:00:15.360542Z","iopub.execute_input":"2024-08-24T13:00:15.360967Z","iopub.status.idle":"2024-08-24T13:02:31.508918Z","shell.execute_reply.started":"2024-08-24T13:00:15.360922Z","shell.execute_reply":"2024-08-24T13:02:31.507546Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.2\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:02:31.511777Z","iopub.execute_input":"2024-08-24T13:02:31.512293Z","iopub.status.idle":"2024-08-24T13:02:44.014666Z","shell.execute_reply.started":"2024-08-24T13:02:31.512250Z","shell.execute_reply":"2024-08-24T13:02:44.013590Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(\"knkarthick/dialogsum\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:02:44.015817Z","iopub.execute_input":"2024-08-24T13:02:44.016093Z","iopub.status.idle":"2024-08-24T13:02:46.885426Z","shell.execute_reply.started":"2024-08-24T13:02:44.016069Z","shell.execute_reply":"2024-08-24T13:02:46.884451Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f212c4b77d4e42259447dc6d6c5be227"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09de860fa695490caae40dc718bdd5b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934d4621b68a4879812a6d0c39566ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54512bff2e7f4ceb8ed33ff281cd030d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1898dfda4d7c4535b17a981dc9db2ea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b88cd09afea4ddcb8d18c377f98fbde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7dc224035d040cca0bf77c24978579e"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:02:46.886694Z","iopub.execute_input":"2024-08-24T13:02:46.886998Z","iopub.status.idle":"2024-08-24T13:02:46.917362Z","shell.execute_reply.started":"2024-08-24T13:02:46.886973Z","shell.execute_reply":"2024-08-24T13:02:46.916267Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_name = '/kaggle/input/flan-t5/pytorch/base/4'\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:02:46.919963Z","iopub.execute_input":"2024-08-24T13:02:46.920272Z","iopub.status.idle":"2024-08-24T13:03:02.617407Z","shell.execute_reply.started":"2024-08-24T13:02:46.920246Z","shell.execute_reply":"2024-08-24T13:03:02.616302Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def print_number_of_traiable_model_parameters(model):\n    trainable_model_parameters = 0\n    all_model_params = 0\n    \n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_parameters += param.numel()\n    return f'Trainable model parameters: {trainable_model_parameters}\\nall model parameters:{all_model_params}\\npercentage of trainable model parameters:{100*trainable_model_parameters/all_model_params:.2f}%'\n\nprint(print_number_of_traiable_model_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:02.618968Z","iopub.execute_input":"2024-08-24T13:03:02.619703Z","iopub.status.idle":"2024-08-24T13:03:02.629186Z","shell.execute_reply.started":"2024-08-24T13:03:02.619666Z","shell.execute_reply":"2024-08-24T13:03:02.628078Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Trainable model parameters: 247577856\nall model parameters:247577856\npercentage of trainable model parameters:100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"index = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors = 'pt').to(device)\noutput = tokenizer.decode(original_model.generate(\n                            inputs[\"input_ids\"],\n                            max_new_tokens = 200,\n                        )[0],\n                         skip_special_tokens = True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'Input prompt:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:02.630886Z","iopub.execute_input":"2024-08-24T13:03:02.631389Z","iopub.status.idle":"2024-08-24T13:03:04.464722Z","shell.execute_reply.started":"2024-08-24T13:03:02.631353Z","shell.execute_reply":"2024-08-24T13:03:04.463626Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nInput prompt:\n\nSummarize the following conversation.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['input_ids'] = example['input_ids'].tolist()\n    example['labels'] = example['labels'].tolist()\n    return example\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:04.466336Z","iopub.execute_input":"2024-08-24T13:03:04.466725Z","iopub.status.idle":"2024-08-24T13:03:18.199379Z","shell.execute_reply.started":"2024-08-24T13:03:04.466690Z","shell.execute_reply":"2024-08-24T13:03:18.198648Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.filter(lambda example, index:index%100 == 0, with_indices = True)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:18.200476Z","iopub.execute_input":"2024-08-24T13:03:18.200744Z","iopub.status.idle":"2024-08-24T13:03:28.725341Z","shell.execute_reply.started":"2024-08-24T13:03:18.200720Z","shell.execute_reply":"2024-08-24T13:03:28.724344Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"print(f'Shape of the dataset:')\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:28.726734Z","iopub.execute_input":"2024-08-24T13:03:28.727061Z","iopub.status.idle":"2024-08-24T13:03:28.733153Z","shell.execute_reply.started":"2024-08-24T13:03:28.727032Z","shell.execute_reply":"2024-08-24T13:03:28.732184Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Shape of the dataset:\nTraining: (125, 2)\nValidation: (5, 2)\nTest: (15, 2)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 125\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 15\n    })\n    validation: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 5\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n\n# Initialize a new W&B run\nwandb.init(project=\"flanT5_summarization\")","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:03:28.734466Z","iopub.execute_input":"2024-08-24T13:03:28.734785Z","iopub.status.idle":"2024-08-24T13:04:54.931721Z","shell.execute_reply.started":"2024-08-24T13:03:28.734751Z","shell.execute_reply":"2024-08-24T13:04:54.930655Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240824_130424-x0fua3l6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/prathamgrover777/flanT5_summarization/runs/x0fua3l6' target=\"_blank\">likely-feather-8</a></strong> to <a href='https://wandb.ai/prathamgrover777/flanT5_summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/prathamgrover777/flanT5_summarization' target=\"_blank\">https://wandb.ai/prathamgrover777/flanT5_summarization</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/prathamgrover777/flanT5_summarization/runs/x0fua3l6' target=\"_blank\">https://wandb.ai/prathamgrover777/flanT5_summarization/runs/x0fua3l6</a>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/prathamgrover777/flanT5_summarization/runs/x0fua3l6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7c5da5b5c970>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"false\"\n\noutput_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n                output_dir = output_dir,\n                learning_rate = 1e-5,\n                num_train_epochs = 1,\n                weight_decay = 0.01,\n                logging_steps = 1,\n                max_steps = 1,\n                report_to=\"wandb\")\n\ntrainer = Trainer(\n        model = original_model,\n        args = training_args,\n        train_dataset = tokenized_datasets['train'],\n        eval_dataset = tokenized_datasets['validation']\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:04:54.933163Z","iopub.execute_input":"2024-08-24T13:04:54.934190Z","iopub.status.idle":"2024-08-24T13:04:54.949369Z","shell.execute_reply.started":"2024-08-24T13:04:54.934158Z","shell.execute_reply":"2024-08-24T13:04:54.948250Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:04:54.950652Z","iopub.execute_input":"2024-08-24T13:04:54.950989Z","iopub.status.idle":"2024-08-24T13:04:56.539729Z","shell.execute_reply.started":"2024-08-24T13:04:54.950962Z","shell.execute_reply":"2024-08-24T13:04:56.538766Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>48.500000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1, training_loss=48.5, metrics={'train_runtime': 1.5583, 'train_samples_per_second': 5.134, 'train_steps_per_second': 0.642, 'total_flos': 5478058819584.0, 'train_loss': 48.5, 'epoch': 0.06})"},"metadata":{}}]},{"cell_type":"code","source":"instruct_model_name= 'truocpham/flan-dialogue-summary-checkpoint'\n\ninstruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:04:56.543368Z","iopub.execute_input":"2024-08-24T13:04:56.543693Z","iopub.status.idle":"2024-08-24T13:05:08.983294Z","shell.execute_reply.started":"2024-08-24T13:04:56.543665Z","shell.execute_reply":"2024-08-24T13:05:08.981956Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f8cd559edae48e59e15ec6b9aa07cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722f79fa4b0842b08f5e1a965b5aedd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1fb7a51c1f49a4a093ff2e35a2be66"}},"metadata":{}}]},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary  = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids.to(device)\n\noriginal_model_outputs = original_model.generate(\n    input_ids = input_ids, \n    generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1)\n)\n\noriginal_model_text_output = tokenizer.decode(\n                                                original_model_outputs[0],\n                                                skip_special_tokens = True\n                                            )\n\n\n\ninstruct_model_outputs = instruct_model.generate(\n    input_ids = input_ids,\n    generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\ninstruct_model_text_output = tokenizer.decode(\n                                                instruct_model_outputs[0], \n                                                skip_special_tokens = True\n                                             )\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:08.987019Z","iopub.execute_input":"2024-08-24T13:05:08.987751Z","iopub.status.idle":"2024-08-24T13:05:10.601957Z","shell.execute_reply.started":"2024-08-24T13:05:08.987703Z","shell.execute_reply":"2024-08-24T13:05:10.600816Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1: I'm thinking about upgrading my computer.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:10.603353Z","iopub.execute_input":"2024-08-24T13:05:10.604084Z","iopub.status.idle":"2024-08-24T13:05:11.469668Z","shell.execute_reply.started":"2024-08-24T13:05:10.604043Z","shell.execute_reply":"2024-08-24T13:05:11.468661Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0759726fb26a41bbb90ef249ad700d18"}},"metadata":{}}]},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\n    Summarize the following conversation.\n    \n    {dialogue}\n    \n    Summary:\"\"\"\n    input_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids.to(device)\n    \n    original_model_outputs = original_model.generate(input_ids = input_ids, generation_config=GenerationConfig(max_new_tokens = 200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens = True)\n    original_model_summaries.append(original_model_text_output)\n    \n    \n    instruct_model_outputs = instruct_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens = True) \n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:11.471003Z","iopub.execute_input":"2024-08-24T13:05:11.471344Z","iopub.status.idle":"2024-08-24T13:05:30.578519Z","shell.execute_reply.started":"2024-08-24T13:05:11.471316Z","shell.execute_reply":"2024-08-24T13:05:30.577326Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  Employees are being advised that all communica...   \n1  This memo is to be distributed to all employee...   \n2  The memo is written by the President of the Of...   \n3  The car is causing a lot of pollution in the c...   \n4  The following questions are about the public t...   \n5  The conversation started with a question about...   \n6               Masha and Hero are getting divorced.   \n7               Masha and Hero are getting divorced.   \n8               Masha and Hero are getting divorced.   \n9  People at the party are celebrating Brian's bi...   \n\n                            instruct_model_summaries  \n0  #Person1# asks Ms. Dawson to take a dictation ...  \n1  #Person1# asks Ms. Dawson to take a dictation ...  \n2  #Person1# asks Ms. Dawson to take a dictation ...  \n3  #Person2# got stuck in traffic again. #Person1...  \n4  #Person2# got stuck in traffic again. #Person1...  \n5  #Person2# got stuck in traffic again. #Person1...  \n6  Masha and Hero are getting divorced. Kate can'...  \n7  Masha and Hero are getting divorced. Kate can'...  \n8  Masha and Hero are getting divorced. Kate can'...  \n9  Brian's birthday is coming. #Person1# invites ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Employees are being advised that all communica...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>This memo is to be distributed to all employee...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>The memo is written by the President of the Of...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>The car is causing a lot of pollution in the c...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>The following questions are about the public t...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>The conversation started with a question about...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>People at the party are celebrating Brian's bi...</td>\n      <td>Brian's birthday is coming. #Person1# invites ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"original_model_results = rouge.compute(\n            predictions = original_model_summaries,\n            references = human_baseline_summaries[0:len(original_model_summaries)],\n            use_aggregator = True,\n            use_stemmer = True)\n\ninstruct_model_results = rouge.compute(\n            predictions = original_model_summaries,\n            references = human_baseline_summaries[0:len(instruct_model_summaries)],\n            use_aggregator = True,\n            use_stemmer = True)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:30.579884Z","iopub.execute_input":"2024-08-24T13:05:30.580237Z","iopub.status.idle":"2024-08-24T13:05:31.119505Z","shell.execute_reply.started":"2024-08-24T13:05:30.580208Z","shell.execute_reply":"2024-08-24T13:05:31.118249Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.31012023869376815, 'rouge2': 0.14129010533979477, 'rougeL': 0.26920964112140583, 'rougeLsum': 0.27194037444037444}\nINSTRUCT MODEL:\n{'rouge1': 0.31012023869376815, 'rouge2': 0.14129010533979477, 'rougeL': 0.26920964112140583, 'rougeLsum': 0.27194037444037444}\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, \n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:31.121187Z","iopub.execute_input":"2024-08-24T13:05:31.121649Z","iopub.status.idle":"2024-08-24T13:05:31.149866Z","shell.execute_reply.started":"2024-08-24T13:05:31.121603Z","shell.execute_reply":"2024-08-24T13:05:31.148701Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"peft_model = get_peft_model(original_model, \n                            lora_config)\nprint(print_number_of_traiable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:31.151036Z","iopub.execute_input":"2024-08-24T13:05:31.154396Z","iopub.status.idle":"2024-08-24T13:05:31.997834Z","shell.execute_reply.started":"2024-08-24T13:05:31.154367Z","shell.execute_reply":"2024-08-24T13:05:31.996540Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Trainable model parameters: 3538944\nall model parameters:251116800\npercentage of trainable model parameters:1.41%\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n                    output_dir = output_dir,\n                    auto_find_batch_size=True,\n                    learning_rate = 1e-3,\n                    num_train_epochs = 1,\n                    logging_steps = 1,\n                    max_steps = 1)\n\n\npeft_trainer = Trainer(\n                model = peft_model,\n                args = peft_training_args,\n                train_dataset = tokenized_datasets['train'])","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:32.001344Z","iopub.execute_input":"2024-08-24T13:05:32.001691Z","iopub.status.idle":"2024-08-24T13:05:32.024535Z","shell.execute_reply.started":"2024-08-24T13:05:32.001661Z","shell.execute_reply":"2024-08-24T13:05:32.023472Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path = '/kaggle/working/peft-dialogue-summary-checkpoint_local'\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:05:32.028010Z","iopub.execute_input":"2024-08-24T13:05:32.028393Z","iopub.status.idle":"2024-08-24T13:05:34.632178Z","shell.execute_reply.started":"2024-08-24T13:05:32.028348Z","shell.execute_reply":"2024-08-24T13:05:34.630930Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>47.750000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/peft-dialogue-summary-checkpoint_local/tokenizer_config.json',\n '/kaggle/working/peft-dialogue-summary-checkpoint_local/special_tokens_map.json',\n '/kaggle/working/peft-dialogue-summary-checkpoint_local/spiece.model',\n '/kaggle/working/peft-dialogue-summary-checkpoint_local/added_tokens.json',\n '/kaggle/working/peft-dialogue-summary-checkpoint_local/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       '/kaggle/working/peft-dialogue-summary-checkpoint_local', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:10:53.113587Z","iopub.execute_input":"2024-08-24T13:10:53.114072Z","iopub.status.idle":"2024-08-24T13:11:00.110913Z","shell.execute_reply.started":"2024-08-24T13:10:53.114040Z","shell.execute_reply":"2024-08-24T13:11:00.109361Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(print_number_of_traiable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:11:27.861233Z","iopub.execute_input":"2024-08-24T13:11:27.861654Z","iopub.status.idle":"2024-08-24T13:11:27.879274Z","shell.execute_reply.started":"2024-08-24T13:11:27.861623Z","shell.execute_reply":"2024-08-24T13:11:27.877610Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Trainable model parameters: 0\nall model parameters:251116800\npercentage of trainable model parameters:0.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_model = peft_model.to(device)\ninstruct_model = instruct_model.to(device)\noriginal_model = original_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:12:17.401003Z","iopub.execute_input":"2024-08-24T13:12:17.401705Z","iopub.status.idle":"2024-08-24T13:12:17.602208Z","shell.execute_reply.started":"2024-08-24T13:12:17.401669Z","shell.execute_reply":"2024-08-24T13:12:17.601012Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids.to(device)\n\n\noriginal_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200, num_beams = 1))\noriginal_mdoel_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens = True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:16:58.350691Z","iopub.execute_input":"2024-08-24T13:16:58.351329Z","iopub.status.idle":"2024-08-24T13:17:00.213578Z","shell.execute_reply.started":"2024-08-24T13:16:58.351296Z","shell.execute_reply":"2024-08-24T13:17:00.212202Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\nPeople at the party are celebrating Brian's birthday.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: #Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}]},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:17:26.906584Z","iopub.execute_input":"2024-08-24T13:17:26.906940Z","iopub.status.idle":"2024-08-24T13:17:55.595298Z","shell.execute_reply.started":"2024-08-24T13:17:26.906913Z","shell.execute_reply":"2024-08-24T13:17:55.594145Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  #Person1: This memo is for internal communicat...   \n1  #Person1#: This is a memo to all employees by ...   \n2  #Person1#: Ms. Dawson, I need you to take a di...   \n3                          The traffic was terrible.   \n4  During the traffic jam, the driver of the car ...   \n5                   The driver was stuck in traffic.   \n6      The divorce is being filed by Masha and Hero.   \n7               Masha and Hero are getting divorced.   \n8               Masha and Hero are getting divorced.   \n9                     Brian's birthday is coming up.   \n\n                            instruct_model_summaries  \\\n0  #Person1# asks Ms. Dawson to take a dictation ...   \n1  #Person1# asks Ms. Dawson to take a dictation ...   \n2  #Person1# asks Ms. Dawson to take a dictation ...   \n3  #Person2# got stuck in traffic again. #Person1...   \n4  #Person2# got stuck in traffic again. #Person1...   \n5  #Person2# got stuck in traffic again. #Person1...   \n6  Masha and Hero are getting divorced. Kate can'...   \n7  Masha and Hero are getting divorced. Kate can'...   \n8  Masha and Hero are getting divorced. Kate can'...   \n9  Brian's birthday is coming. #Person1# invites ...   \n\n                                peft_model_summaries  \n0  The memo is to be distributed to all employees...  \n1  The memo is to be distributed to all employees...  \n2  The memo is to be distributed to all employees...  \n3  The traffic jam at the Carrefour intersection ...  \n4  The traffic jam at the Carrefour intersection ...  \n5  The traffic jam at the Carrefour intersection ...  \n6               Masha and Hero are getting divorced.  \n7               Masha and Hero are getting divorced.  \n8               Masha and Hero are getting divorced.  \n9                     Brian's birthday is coming up.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1: This memo is for internal communicat...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>The memo is to be distributed to all employees...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>#Person1#: This is a memo to all employees by ...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>The memo is to be distributed to all employees...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>The memo is to be distributed to all employees...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>The traffic was terrible.</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>During the traffic jam, the driver of the car ...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>The driver was stuck in traffic.</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>The divorce is being filed by Masha and Hero.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced. Kate can'...</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Brian's birthday is coming up.</td>\n      <td>Brian's birthday is coming. #Person1# invites ...</td>\n      <td>Brian's birthday is coming up.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:17:55.597275Z","iopub.execute_input":"2024-08-24T13:17:55.597612Z","iopub.status.idle":"2024-08-24T13:17:56.599848Z","shell.execute_reply.started":"2024-08-24T13:17:55.597584Z","shell.execute_reply":"2024-08-24T13:17:56.598618Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2437558577776753, 'rouge2': 0.0998803418803419, 'rougeL': 0.2086769255063286, 'rougeLsum': 0.21380618156982634}\nINSTRUCT MODEL:\n{'rouge1': 0.4015906463624618, 'rouge2': 0.17568542724181807, 'rougeL': 0.2874569966059625, 'rougeLsum': 0.2886327613084294}\nPEFT MODEL:\n{'rouge1': 0.26109650997150996, 'rouge2': 0.11055072463768116, 'rougeL': 0.2302777777777778, 'rougeLsum': 0.2339245014245014}\n","output_type":"stream"}]}]}